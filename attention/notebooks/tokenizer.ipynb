{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install huggingface","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:16:37.294068Z","iopub.execute_input":"2024-03-19T15:16:37.294467Z","iopub.status.idle":"2024-03-19T15:16:52.270409Z","shell.execute_reply.started":"2024-03-19T15:16:37.294434Z","shell.execute_reply":"2024-03-19T15:16:52.269296Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import Any\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:16:52.272634Z","iopub.execute_input":"2024-03-19T15:16:52.272938Z","iopub.status.idle":"2024-03-19T15:16:56.242341Z","shell.execute_reply.started":"2024-03-19T15:16:52.272908Z","shell.execute_reply":"2024-03-19T15:16:56.241339Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_sentences(dataset, lang):\n    for data in dataset: \n        yield item['translation'][lang]","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:16:56.243713Z","iopub.execute_input":"2024-03-19T15:16:56.244167Z","iopub.status.idle":"2024-03-19T15:16:56.248996Z","shell.execute_reply.started":"2024-03-19T15:16:56.244142Z","shell.execute_reply":"2024-03-19T15:16:56.247983Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":" def get_or_build_tokenizer(config, ds, lang): \n        tokenizer_path = Path(config['tokenizer_file'].format(lang))\n        if not Path.exists(tokenizer_path):\n            tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n            tokenizer.pre_tokenizer = Whitespace()\n            trainer = WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n            tokenizer.train_from_iterator(get_sentences(ds, lang))\n            tokenizer.save(str(tokenizer_path))\n        else: \n            tokenizer = Tokenzier.from_file(str(tokenizer_path))\n        return tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:16:56.250084Z","iopub.execute_input":"2024-03-19T15:16:56.250337Z","iopub.status.idle":"2024-03-19T15:16:56.264673Z","shell.execute_reply.started":"2024-03-19T15:16:56.250316Z","shell.execute_reply":"2024-03-19T15:16:56.263714Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_dataset(config): \n    raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n    \n    tokenizer_src = get_or_build_tokenizer(config, raw, config[\"lang_src\"])\n    tokenizer_tgt = get_or_build_tokenizer(config, raw, config[\"lang_tgt\"])\n    \n    train_ds = len(0.9 * len(raw))\n    val_ds = len(raw) - train_ds\n    train_raw, val_raw = random_split(raw, [train_ds, val_ds])\n    \n    train_ds = BilingualDataset(train_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    \n    max_src = 0 \n    max_tgt = 0\n    for item in raw: \n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_src.encode(item['translation'][config['lang_tgt']]).ids\n        max_src = max(max_src, len(src_ids))\n        max_tgt = max(max_tgt, len(tgt_ids))\n        \n    print(f'Max length of source: {max_src}')\n    print(f'Max length of target: {max_tgt}')\n    \n    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)\n    \n    return train_loader, val_loader, tokenizer_src, tokenizer_tgt","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:50:07.173627Z","iopub.execute_input":"2024-03-19T15:50:07.174460Z","iopub.status.idle":"2024-03-19T15:50:07.183940Z","shell.execute_reply.started":"2024-03-19T15:50:07.174425Z","shell.execute_reply":"2024-03-19T15:50:07.183028Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len): \n    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:51:50.878738Z","iopub.execute_input":"2024-03-19T15:51:50.879455Z","iopub.status.idle":"2024-03-19T15:51:50.884472Z","shell.execute_reply.started":"2024-03-19T15:51:50.879422Z","shell.execute_reply":"2024-03-19T15:51:50.883498Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def causal_mask(size): \n    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:16:56.284408Z","iopub.execute_input":"2024-03-19T15:16:56.284747Z","iopub.status.idle":"2024-03-19T15:16:56.295654Z","shell.execute_reply.started":"2024-03-19T15:16:56.284724Z","shell.execute_reply":"2024-03-19T15:16:56.294760Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class BilingualDataset(Dataset): \n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None: \n        super().__init__()\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        \n        self.eos_token = torch.Tensor([tokenizer_src.token_to_id(['[EOS]'])], dtype=torch.int64)\n        self.sos_token = torch.Tensor([tokenizer_src.token_to_id(['[SOS]'])], dtype=torch.int64)\n        self.pad_token = torch.Tensor([tokenizer_src.token_to_id(['[PAD]'])], dtype=torch.int64)\n    \n    def __len__(self): \n        return len(self.ds)\n    \n    def __getitem__(self, index: Any) -> Any: \n        src_pair = self.ds[index]\n        src_text = src_pair['translation'][src_lang]\n        tgt_text = src_pair['translation'][tgt_lang]\n        \n        enc_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_tokens = self.tokenizer_src.decode(tgt_text).ids\n        \n        enc_padding = self.seq_len - len(enc_tokens) - 2\n        dec_padding = self.seq_len - len(dec_tokens) - 1\n        \n        if enc_padding < 0 or dec_padding < 0: \n            raise ValueError('Too long')\n        \n        enc_input = torch.cat(\n        [\n            self.sos_token, \n            torch.tensor(enc_tokens, dtype=torch.int64), \n            self.eos_token, \n            torch.tensor([self.pad_token] * enc_padding, type=torch.int64)\n        ])\n        \n        dec_input = torch.cat(\n        [\n            self.sos_token, \n            torch.tensor(dec_input, dtype=torch.int64), \n            torch.tensor([self.pad_token] * dec_padding, type=torch.int64)\n        ])\n        \n        label = torch.cat(\n        [\n            torch.tensor(dec_input, type=torch.int64), \n            self.eos_token, \n            torch.tensor([self.pad_token] * dec_padding, type=torch.int64)\n        ])\n        \n        assert enc_input.size(0) == self.seq_len\n        assert dec_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n        \n        return {\n            \"encoder_input\": enc_input, \n            \"decoder_input\": dec_input, \n            \"encoder_mask\": (enc_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), \n            \"decoder_mask\": (dec_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(dec_input.size(0)),\n            \"label\": label,\n            \"src_text\": src_text, \n            \"tgt_text\": tgt_text\n        }","metadata":{"execution":{"iopub.status.busy":"2024-03-19T15:16:56.296843Z","iopub.execute_input":"2024-03-19T15:16:56.297103Z","iopub.status.idle":"2024-03-19T15:16:56.313715Z","shell.execute_reply.started":"2024-03-19T15:16:56.297081Z","shell.execute_reply":"2024-03-19T15:16:56.312862Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}