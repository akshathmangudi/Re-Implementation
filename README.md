# Re-Implementation

A repository that holds my implementation of machine learning papers.

Current re-implementations are:

| Paper                                                                                                          | Code                 | Status    |
|----------------------------------------------------------------------------------------------------------------|----------------------|-----------|
| [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) | [ViT](./src/vit)     | Completed |
| [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)                               | [ResNet](./src/resnet) | Completed |
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762) | [Transformers](./src/attention) | Close to Completion |
| [Global Context Vision Transformers](https://arxiv.org/abs/2206.09959) | [GCViT](./src/gcvit) | In Progress |
| [Accelerating the Super-Resolution Convolutional Network](https://arxiv.org/abs/1608.00367) | [FSRCNN](./src/fsrcnn) | In Progress |
| [T-Rex: Counting by Visual Prompting](https://arxiv.org/abs/2311.13596) | [T-Rex](./src/trex) | In Progress |
| [Swin Transformer: Heirarchial Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) | [Swin](./src/swin) | In Progress |
| [Autoencoders](https://arxiv.org/abs/2003.05991) | [Autoencoders](./src/autoencoders) | In Progress |

## Contributions:

These are my personal implementations in order to educate myself. That being said, if there are any issues with the
code, such as incorrect math,
not enough comments or documentation, or poor modularity, please create an issue so I can review and make changes. Pull
requests must be the last resort.

## Licensing:

This repository is under the MIT License. See the LICENSE file for more details.