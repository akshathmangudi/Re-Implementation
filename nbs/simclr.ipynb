{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9iegpycmERxY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-vD6MY20Xmvf"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Logging config\n",
        "def setup_logging(log_file='simclr_training.log'):\n",
        "    logging.basicConfig(\n",
        "        filename=log_file,\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    console = logging.StreamHandler()\n",
        "    console.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "    console.setFormatter(formatter)\n",
        "    logging.getLogger('').addHandler(console)\n",
        "\n",
        "setup_logging()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z676t9rdEz4b"
      },
      "outputs": [],
      "source": [
        "# We will implement our loss function to pass it through our CIFAR-10 dataset.\n",
        "def nt_xent_loss(z1, z2, temperature=0.5):\n",
        "    N = z1.size(0)\n",
        "    z = torch.cat([z1, z2], dim=0)  # (2N, D)\n",
        "    z = F.normalize(z, dim=1)\n",
        "\n",
        "    sim_matrix = torch.matmul(z, z.T) / temperature\n",
        "\n",
        "    # Mask self-similarity\n",
        "    mask = torch.eye(2*N, device=z.device).bool()\n",
        "    sim_matrix.masked_fill_(mask, -9e15)\n",
        "\n",
        "    # Positive indices: (i, i+N) and (i+N, i)\n",
        "    positives = torch.cat([\n",
        "        torch.arange(N, device=z.device) + N,\n",
        "        torch.arange(N, device=z.device)\n",
        "    ])\n",
        "\n",
        "    pos_sim = sim_matrix[torch.arange(2*N), positives]  # (2N,)\n",
        "    numerator = torch.exp(pos_sim)\n",
        "    denominator = torch.exp(sim_matrix).sum(dim=1)\n",
        "\n",
        "    loss = -torch.log(numerator / denominator)\n",
        "    return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrSTj7wGIXuT",
        "outputId": "182f305d-9c24-4761-89be-7ccc6e8afaea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed successfully.\n"
          ]
        }
      ],
      "source": [
        "def test_nt_xent_loss():\n",
        "    torch.manual_seed(42)\n",
        "    # Test 1: Identical views → low-ish loss\n",
        "    z = F.normalize(torch.randn(8, 128), dim=1)\n",
        "    loss = nt_xent_loss(z, z)\n",
        "    assert loss.item() < 1.2, f\"Expected reasonably low loss for identical views, got {loss.item()}\"\n",
        "\n",
        "\n",
        "    # Test 2: Symmetric property\n",
        "    z1 = F.normalize(torch.randn(16, 128), dim=1)\n",
        "    z2 = F.normalize(torch.randn(16, 128), dim=1)\n",
        "    loss1 = nt_xent_loss(z1, z2)\n",
        "    loss2 = nt_xent_loss(z2, z1)\n",
        "    assert torch.allclose(loss1, loss2, atol=1e-5), f\"Loss not symmetric: {loss1.item()} vs {loss2.item()}\"\n",
        "\n",
        "    # Test 3: Random projections → moderate/high loss\n",
        "    z1 = F.normalize(torch.randn(64, 128), dim=1)\n",
        "    z2 = F.normalize(torch.randn(64, 128), dim=1)\n",
        "    loss = nt_xent_loss(z1, z2)\n",
        "    assert loss.item() > 2.0, f\"Expected high loss for random vectors, got {loss.item()}\"\n",
        "\n",
        "    # Test 4: Batch size 1 → should raise or return a valid result\n",
        "    try:\n",
        "        z1 = F.normalize(torch.randn(1, 128), dim=1)\n",
        "        z2 = F.normalize(torch.randn(1, 128), dim=1)\n",
        "        loss = nt_xent_loss(z1, z2)\n",
        "        assert not torch.isnan(loss), \"Loss is NaN for batch size 1\"\n",
        "    except Exception as e:\n",
        "        print(f\"Expected failure on batch=1: {e}\")\n",
        "\n",
        "    print(\"All tests passed successfully.\")\n",
        "\n",
        "# Run tests\n",
        "test_nt_xent_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_LtEZsyXWLjJ"
      },
      "outputs": [],
      "source": [
        "# SimCLR training requires two augmented views of the same sample.\n",
        "class SimCLRDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, _ = self.base_dataset[index]\n",
        "        return self.transform(x), self.transform(x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wv7Z_sIoW67v"
      },
      "outputs": [],
      "source": [
        "# Augmenting the images\n",
        "def get_simclr_augmentations():\n",
        "    return transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.GaussianBlur(kernel_size=3),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "def get_cifar10_dataloader(batch_size=256):\n",
        "    transform = get_simclr_augmentations()\n",
        "    base_dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
        "    dataset = SimCLRDataset(base_dataset, transform)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fpe6bVu2XOO9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_simclr(model, data_loader, optimizer, device, temperature=0.5, epoch=1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    loop = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch}\", leave=False)\n",
        "\n",
        "    for batch_idx, (x1, x2) in loop:\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "\n",
        "        z1 = model(x1)\n",
        "        z2 = model(x2)\n",
        "\n",
        "        loss = nt_xent_loss(z1, z2, temperature)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "\n",
        "        loop.set_postfix(loss=loss.item(), avg_loss=avg_loss)\n",
        "        if batch_idx % 10 == 0:\n",
        "            logging.info(f\"Epoch [{epoch}] Batch [{batch_idx}/{len(data_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    logging.info(f\"Epoch [{epoch}] Completed. Avg Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "O7qx-B95XQLV",
        "outputId": "1cdfaf96-6d4d-473b-adac-4302617d55e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5423816851d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_simclr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-06705274766a>\u001b[0m in \u001b[0;36mtrain_simclr\u001b[0;34m(model, data_loader, optimizer, device, temperature, epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = SimCLR().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    dataloader = get_cifar10_dataloader(batch_size=256)\n",
        "\n",
        "    for epoch in range(1, 11):\n",
        "        train_simclr(model, dataloader, optimizer, device, temperature=0.5, epoch=epoch)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
