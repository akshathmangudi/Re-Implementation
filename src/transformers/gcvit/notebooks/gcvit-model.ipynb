{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt \nimport random\nfrom PIL import Image\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10 \n\nnumpy.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-28T08:48:56.166372Z","iopub.execute_input":"2023-12-28T08:48:56.167042Z","iopub.status.idle":"2023-12-28T08:48:56.177573Z","shell.execute_reply.started":"2023-12-28T08:48:56.167007Z","shell.execute_reply":"2023-12-28T08:48:56.176319Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\ndef download_data(test, transform, batch_size, shuffle, num_workers): \n    if test: \n        test_set = CIFAR10(root=\"./cifar10\", train=False, download=True, transform=transform)\n        test_loader = DataLoader(test_set, batch_size, shuffle=shuffle, num_workers=num_workers)\n    else: \n        train_set = CIFAR10(root=\"./cifar10\", train=True, download=True, transform=transform)\n        train_loader = DataLoader(train_set, batch_size, shuffle=shuffle, num_workers=num_workers)\n        \n    return train_loader, test_loader\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:56.179337Z","iopub.execute_input":"2023-12-28T08:48:56.179748Z","iopub.status.idle":"2023-12-28T08:48:56.186717Z","shell.execute_reply.started":"2023-12-28T08:48:56.179714Z","shell.execute_reply":"2023-12-28T08:48:56.185595Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\ndef download_data(test, transform, batch_size, shuffle, num_workers): \\n    if test: \\n        test_set = CIFAR10(root=\"./cifar10\", train=False, download=True, transform=transform)\\n        test_loader = DataLoader(test_set, batch_size, shuffle=shuffle, num_workers=num_workers)\\n    else: \\n        train_set = CIFAR10(root=\"./cifar10\", train=True, download=True, transform=transform)\\n        train_loader = DataLoader(train_set, batch_size, shuffle=shuffle, num_workers=num_workers)\\n        \\n    return train_loader, test_loader\\n'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\ndef transform_data(): \n    transform = transforms.Compose(\n    [\n        transforms.ToTensor(), \n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    train_data = download_data(test=False, transform=transform, batch_size=4, shuffle=True, num_workers=2)\n    test_data = download_data(test=True, transform=transform, batch_size=4, shuffle=False, num_workers=2)\n    \n    return train_data, test_data\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:56.192655Z","iopub.execute_input":"2023-12-28T08:48:56.192929Z","iopub.status.idle":"2023-12-28T08:48:56.199682Z","shell.execute_reply.started":"2023-12-28T08:48:56.192906Z","shell.execute_reply":"2023-12-28T08:48:56.198632Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'\\ndef transform_data(): \\n    transform = transforms.Compose(\\n    [\\n        transforms.ToTensor(), \\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n    ])\\n    \\n    train_data = download_data(test=False, transform=transform, batch_size=4, shuffle=True, num_workers=2)\\n    test_data = download_data(test=True, transform=transform, batch_size=4, shuffle=False, num_workers=2)\\n    \\n    return train_data, test_data\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# transform_data()","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:56.201420Z","iopub.execute_input":"2023-12-28T08:48:56.201778Z","iopub.status.idle":"2023-12-28T08:48:56.206840Z","shell.execute_reply.started":"2023-12-28T08:48:56.201751Z","shell.execute_reply":"2023-12-28T08:48:56.205831Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef show_image(classes, img): \n    img = img / 2 + 0.5\n    numpy_img = img.numpy()\n    plt.imshow(numpy.transpose(numpy_img, (1, 2, 0)))\n    plt.show()\n    \ndata_loop = iter(train_data)\nimages, labels = next(data_loop)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:56.208626Z","iopub.execute_input":"2023-12-28T08:48:56.209439Z","iopub.status.idle":"2023-12-28T08:48:56.217530Z","shell.execute_reply.started":"2023-12-28T08:48:56.209413Z","shell.execute_reply":"2023-12-28T08:48:56.216508Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'\\ndef show_image(classes, img): \\n    img = img / 2 + 0.5\\n    numpy_img = img.numpy()\\n    plt.imshow(numpy.transpose(numpy_img, (1, 2, 0)))\\n    plt.show()\\n    \\ndata_loop = iter(train_data)\\nimages, labels = next(data_loop)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(), \n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_set = CIFAR10(root=\"./cifar10\", train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=2)    \n\ntest_set = CIFAR10(root=\"./cifar10\", train=False, download=True, transform=transform)\ntest_loader = DataLoader(test_set, batch_size=4, shuffle=False, num_workers=2)\n\ntrain_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:56.219009Z","iopub.execute_input":"2023-12-28T08:48:56.219311Z","iopub.status.idle":"2023-12-28T08:48:57.913367Z","shell.execute_reply.started":"2023-12-28T08:48:56.219278Z","shell.execute_reply":"2023-12-28T08:48:57.912303Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x7d94b3356e00>,\n <torch.utils.data.dataloader.DataLoader at 0x7d94b3357b80>)"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\ndef show_image(image, classes): \n    img = image / 2 + 0.5 \n    np_img = img.numpy()\n    plt.imshow(numpy.transpose(np_img, (1, 2, 0)))\n    plt.show()\n    \n\ndata_iter = iter(train_loader)\nimages, labels = next(data_iter)\nbatch_size = 4\n\nclasses = {'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}\n\nshow_image(torchvision.utils.make_grid(images), classes)\nprint(' '.join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:57.914900Z","iopub.execute_input":"2023-12-28T08:48:57.915202Z","iopub.status.idle":"2023-12-28T08:48:57.921920Z","shell.execute_reply.started":"2023-12-28T08:48:57.915177Z","shell.execute_reply":"2023-12-28T08:48:57.920763Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\ndef show_image(image, classes): \\n    img = image / 2 + 0.5 \\n    np_img = img.numpy()\\n    plt.imshow(numpy.transpose(np_img, (1, 2, 0)))\\n    plt.show()\\n    \\n\\ndata_iter = iter(train_loader)\\nimages, labels = next(data_iter)\\nbatch_size = 4\\n\\nclasses = {\\'plane\\', \\'car\\', \\'bird\\', \\'cat\\', \\'deer\\', \\'dog\\', \\'frog\\', \\'horse\\', \\'ship\\', \\'truck\\'}\\n\\nshow_image(torchvision.utils.make_grid(images), classes)\\nprint(\\' \\'.join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))\\n'"},"metadata":{}}]},{"cell_type":"code","source":"class MSA(nn.Module): \n    def __init__(self, d, n_heads=4): \n        super(MSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads \n        \n        assert d % n_heads == 0\n        \n        d_head = int(d / n_heads)\n        self.q_mapping = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mapping = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mapping = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n    \n    def forward(self, sequence): \n        result = []\n        for sequence in sequences: \n            seq_result = []\n            for head in range(self.n_heads): \n                q_mapping = self.q_mapping[head]\n                k_mapping = self.k_mapping[head]\n                v_mapping = self.v_mapping[head]\n                \n                seq = sequence[:, head * self.d_head: (head+1) * self.d_head]\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n                \n                attention = self.softmax(q @ K.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n            result.append(torch.hstack(seq_result))\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:57.923520Z","iopub.execute_input":"2023-12-28T08:48:57.923868Z","iopub.status.idle":"2023-12-28T08:48:57.936789Z","shell.execute_reply.started":"2023-12-28T08:48:57.923841Z","shell.execute_reply":"2023-12-28T08:48:57.935932Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module): \n    def __init__(self, in_features, hidden_features=None, out_features=None, activation_layer=nn.GELU, drop=0.): \n        super(MLP, self).__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.layer1 = nn.Linear(in_features, hidden_features)\n        self.activation = activation_layer()\n        self.layer2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    \n    def forward(self, x): \n        x = self.layer1(x)\n        x = self.activation(x)\n        x = self.drop(x)\n        x = self.layer2(x)\n        x = self.drop(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:57.938945Z","iopub.execute_input":"2023-12-28T08:48:57.939326Z","iopub.status.idle":"2023-12-28T08:48:57.947693Z","shell.execute_reply.started":"2023-12-28T08:48:57.939299Z","shell.execute_reply":"2023-12-28T08:48:57.946743Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class SE(nn.Module): \n    def __init__(self, channels, reduction): \n        super(SE, self).__init__()\n        mid_channels = channels // reduction \n        self.squeeze = nn.AdaptiveMaxPool2dAd(output_size=1)\n        self.excitation = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, kernel_size=1)\n            nn.SiLU(), \n            nn.Conv2d(channels // reduction, channels, kernel_size=1)\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x): \n        s = self.squeeze(x)\n        e = self.excitation(s)\n        return x * e","metadata":{"execution":{"iopub.status.busy":"2023-12-28T08:48:57.949046Z","iopub.execute_input":"2023-12-28T08:48:57.949407Z","iopub.status.idle":"2023-12-28T08:48:57.960305Z","shell.execute_reply.started":"2023-12-28T08:48:57.949380Z","shell.execute_reply":"2023-12-28T08:48:57.959291Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=1, bias=False, bn=True, act=True): \n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, groups=groups, bias=bias), \n        nn.BatchNorm2d(out_channels) if bn else nn.Identity()\n        nn.SiLU() if act else nn.Identity()\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FusedMBConv(nn.Module): \n    def __init__(self, in_channels, out_channels, expansion, kernel_size=3, stride=1, bn=True, act=True, r=24, dropout=0.1): \n        super(FusedMBConv, self).__init__()\n        self.skip_connection = (in_channels == out_channels) and (stride == 1) \n        padding = (kernel_size - 1) // 2\n        expanded = expansion * in_channels\n        \n        self.expand_conv = conv_block(in_channels, expanded, kernel_size=3, stride=stride, padding=1)\n        self.point_conv = conv_block(expanded, out_channels, kernel_size=1, padding=0, act=False)\n        \n        if expansion == 1: \n            self.point_pw = nn.Identity()\n            self.expand_pw = conv_block(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x): \n        res = x \n        x = self.expand_pw(x)\n        x = self.reduce_pw(x)\n        if self.skip_connection: \n            x = self.dropout(x)\n            x = x + res\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-28T09:18:48.391572Z","iopub.execute_input":"2023-12-28T09:18:48.391935Z","iopub.status.idle":"2023-12-28T09:18:48.401628Z","shell.execute_reply.started":"2023-12-28T09:18:48.391908Z","shell.execute_reply":"2023-12-28T09:18:48.400533Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class FeatExtract(nn.Module): \n    def __init__(self, dim, keep_dim=False): \n        super(FeatExtract, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=False)\n            nn.GELU(), \n            SE(dim, dim),\n            nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, bias=False)\n        )\n        \n        if not keep_dim: \n            self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.keep_dim = keep_dim \n        \n    def forward(self, x): \n        x = x.contiguous()\n        x = x + self.conv(x)\n        if not self.keep_dim: \n            x = self.pool(x) \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-12-28T10:16:28.115241Z","iopub.execute_input":"2023-12-28T10:16:28.115922Z","iopub.status.idle":"2023-12-28T10:16:28.124800Z","shell.execute_reply.started":"2023-12-28T10:16:28.115888Z","shell.execute_reply":"2023-12-28T10:16:28.123598Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[23], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=False)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"],"ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (3576103443.py, line 5)","output_type":"error"}]}]}