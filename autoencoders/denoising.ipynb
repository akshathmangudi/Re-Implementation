{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OOKG8hm-WLCi",
        "outputId": "7006b73c-240d-458b-b4ed-367f288d374c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Regularized Autoencoders\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIBjQUs0WPWo",
        "outputId": "e0b3720c-d596-419e-bc28-61c6a9fab19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 450504526.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 111851979.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 219768387.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 21747178.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# We'll use the MNIST dataset\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\",\n",
        "                         train=True,\n",
        "                         download=True,\n",
        "                         transform=transform)\n",
        "\n",
        "test_dataset = datasets.MNIST(root=\"./data\",\n",
        "                              train=False,\n",
        "                              download=True,\n",
        "                              transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=32,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                          batch_size=32,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oBO-VcnwZSws",
        "outputId": "8e3033bc-4d66-4321-8305-1b5e51cb35ea"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntransform = transforms.ToTensor()\\n\\ntrain_data = datasets.CIFAR10(root=\"./data\", \\n                              train=True, \\n                              download=True, \\n                              transform=transform)\\n\\ntest_data = datasets.CIFAR10(root=\"./data\", \\n                             train=False, \\n                             download=True, \\n                             transform=transform)\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.CIFAR10(root=\"./data\",\n",
        "                              train=True,\n",
        "                              download=True,\n",
        "                              transform=transform)\n",
        "\n",
        "test_data = datasets.CIFAR10(root=\"./data\",\n",
        "                             train=False,\n",
        "                             download=True,\n",
        "                             transform=transform)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "RGehszekZzQA",
        "outputId": "94272ede-f4a1-4e9d-8eae-db770a2fad61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nbatch_size = 128\\nnum_workers = 0\\n \\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \\n                                           num_workers=num_workers, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \\n                                           num_workers=num_workers, shuffle=False)\\n\\nclasses = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\\n\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "batch_size = 128\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                           num_workers=num_workers, shuffle=False)\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yq32FTz7WVNm"
      },
      "outputs": [],
      "source": [
        "class RegularizedAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(32,64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e-CzX9xkWWfR"
      },
      "outputs": [],
      "source": [
        "ae_model = RegularizedAE().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(ae_model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PZxsnMUjWmdq"
      },
      "outputs": [],
      "source": [
        "def create_noise(images, noise_factor):\n",
        "    return images * (1 - noise_factor) + torch.rand(images.size()).to(device) * noise_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7Ebur4bWXfy",
        "outputId": "5fd2fd30-fb5d-4a88-d837-1a901e8c86bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 of 10, Loss: 0.01571075299580892\n",
            "Epoch 2 of 10, Loss: 0.015378648275633653\n",
            "Epoch 3 of 10, Loss: 0.015158126931885878\n",
            "Epoch 4 of 10, Loss: 0.01497927515655756\n",
            "Epoch 5 of 10, Loss: 0.01485109131783247\n",
            "Epoch 6 of 10, Loss: 0.014717620947957038\n",
            "Epoch 7 of 10, Loss: 0.014612542220950127\n",
            "Epoch 8 of 10, Loss: 0.014525073391199112\n",
            "Epoch 9 of 10, Loss: 0.014457494953771433\n",
            "Epoch 10 of 10, Loss: 0.014388798725605011\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "noise_factor = 0.1\n",
        "\n",
        "# We are artificially adding noise to the dataset and pass in to our AE.\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss =  0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        images, _ = data\n",
        "        images = images.to(device)  # Move images to the device\n",
        "        images = images.view(images.size(0), -1)\n",
        "        noisy_imgs = create_noise(images, noise_factor)\n",
        "        noisy_imag = noisy_imgs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ae_model(noisy_imgs)\n",
        "        loss = criterion(outputs, images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*images.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.dataset)  # Correct calculation of average loss\n",
        "    print(f\"Epoch {epoch + 1} of {n_epochs}, Loss: {train_loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
